{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a00fb8",
   "metadata": {},
   "source": [
    "This code is used for kaggle training at URL:https://www.kaggle.com/code/nocharon/nbme-nlp\n",
    "https://www.kaggle.com/code/nocharon/dataaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35519a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87795c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "class NBMEDataProcessor:\n",
    "    \"\"\"\n",
    "    NBME Clinical Patient Notes Data Processor, adapted for Kaggle Notebook paths\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir=None, output_dir=None, n_folds=5):\n",
    "        \"\"\"\n",
    "        Initialize NBME data processor\n",
    "\n",
    "        Args:\n",
    "            data_dir: Input data directory, defaults to automatically detect Kaggle mount path\n",
    "            output_dir: Processed data output directory, defaults to '/kaggle/working/processed'\n",
    "            n_folds: Number of cross-validation folds\n",
    "        \"\"\"\n",
    "        # Kaggle dataset default mount path\n",
    "        kaggle_dir = '/kaggle/input/nbme-score-clinical-patient-notes'\n",
    "        # Working directory output path\n",
    "        default_output = '/kaggle/working/processed'\n",
    "\n",
    "        # Automatically detect data directory\n",
    "        if data_dir:\n",
    "            self.data_dir = data_dir\n",
    "        elif os.path.exists(kaggle_dir):\n",
    "            self.data_dir = kaggle_dir\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Data directory not found: {kaggle_dir}\")\n",
    "\n",
    "        # Set output directory\n",
    "        self.output_dir = output_dir or default_output\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        self.patient_notes = None\n",
    "        self.features = None\n",
    "        self.train_processed = None\n",
    "        self.final_data = None\n",
    "        self.n_folds = n_folds\n",
    "        self.feature_female = []\n",
    "        self.feature_male = []\n",
    "        self.feature_year = []\n",
    "\n",
    "        # Ignore warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        # Medical abbreviation dictionary\n",
    "        self.medical_abbreviations = {\n",
    "            'htn': 'hypertension', 'dm': 'diabetes mellitus', 'chf': 'congestive heart failure',\n",
    "            'cad': 'coronary artery disease', 'mi': 'myocardial infarction', 'afib': 'atrial fibrillation',\n",
    "            'copd': 'chronic obstructive pulmonary disease', 'uti': 'urinary tract infection',\n",
    "            'bph': 'benign prostatic hyperplasia', 'gerd': 'gastroesophageal reflux disease',\n",
    "            'hx': 'history', 'yo': 'year old', 'y/o': 'year old', 'yo/': 'year old', 'y.o.': 'year old',\n",
    "            'w/': 'with', 's/p': 'status post', 'h/o': 'history of', 'c/o': 'complains of',\n",
    "            'p/w': 'presenting with', 'neg': 'negative', 'pos': 'positive', '+': 'positive', '-': 'negative',\n",
    "            'w/o': 'without', 'b/l': 'bilateral', 'r/o': 'rule out', '&': 'and', 'pt': 'patient',\n",
    "            'sx': 'symptoms', 'dx': 'diagnosis', 'tx': 'treatment', 'fx': 'fracture', 'vs': 'vital signs',\n",
    "        }\n",
    "        print(f\"[INFO] Data directory: {self.data_dir}\")\n",
    "        print(f\"[INFO] Output directory: {self.output_dir}\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load all necessary data files\"\"\"\n",
    "        print(\"Loading data files from:\", self.data_dir)\n",
    "        print(\"Contents:\", os.listdir(self.data_dir))\n",
    "        self.train = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "        self.patient_notes = pd.read_csv(os.path.join(self.data_dir, 'patient_notes.csv'))\n",
    "        self.features = pd.read_csv(os.path.join(self.data_dir, 'features.csv'))\n",
    "        test_path = os.path.join(self.data_dir, 'test.csv')\n",
    "        if os.path.exists(test_path):\n",
    "            self.test = pd.read_csv(test_path)\n",
    "        print(f\"Loaded train ({self.train.shape}), notes ({self.patient_notes.shape}), features ({self.features.shape})\")\n",
    "        if self.test is not None:\n",
    "            print(f\"Loaded test ({self.test.shape})\")\n",
    "        self.identify_feature_types()\n",
    "            \n",
    "    def identify_feature_types(self):\n",
    "        \"\"\"Identify different types of features (gender, age, etc.)\"\"\"\n",
    "        print(\"Identifying feature types...\")\n",
    "        \n",
    "        # Reset feature type lists\n",
    "        self.feature_female = []\n",
    "        self.feature_male = []\n",
    "        self.feature_year = []\n",
    "        \n",
    "        # Iterate through features\n",
    "        for idx, row in self.features.iterrows():\n",
    "            feature_text = row['feature_text'].lower()\n",
    "            feature_num = row['feature_num']\n",
    "            \n",
    "            # Identify female-related features\n",
    "            if any(term in feature_text for term in ['female', 'woman', 'girl', 'mother', 'sister', 'daughter']):\n",
    "                self.feature_female.append(feature_num)\n",
    "                \n",
    "            # Identify male-related features\n",
    "            if any(term in feature_text for term in ['male', 'man', 'boy', 'father', 'brother', 'son']):\n",
    "                self.feature_male.append(feature_num)\n",
    "                \n",
    "            # Identify age-related features\n",
    "            if any(term in feature_text for term in ['age', 'year old', 'y.o', 'yo', 'y/o']):\n",
    "                self.feature_year.append(feature_num)\n",
    "                \n",
    "        print(f\"Identified {len(self.feature_female)} female-related features\")\n",
    "        print(f\"Identified {len(self.feature_male)} male-related features\")\n",
    "        print(f\"Identified {len(self.feature_year)} age-related features\")\n",
    "    \n",
    "    def preprocess_features(self):\n",
    "        \"\"\"Process special cases in feature text (from second notebook)\"\"\"\n",
    "        # Fix text for feature #27\n",
    "        self.features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "        # Additional feature preprocessing logic can be added here\n",
    "        return self.features\n",
    "    \n",
    "    def parse_annotations(self):\n",
    "        \"\"\"Convert string format annotations and locations to lists\"\"\"\n",
    "        # Ensure annotations and locations are parsed into Python objects\n",
    "        if isinstance(self.train['annotation'].iloc[0], str):\n",
    "            self.train['annotation'] = self.train['annotation'].apply(ast.literal_eval)\n",
    "        \n",
    "        if isinstance(self.train['location'].iloc[0], str):\n",
    "            self.train['location'] = self.train['location'].apply(ast.literal_eval)\n",
    "            \n",
    "        # Add annotation length field\n",
    "        self.train['annotation_length'] = self.train['annotation'].apply(len)\n",
    "        return self.train\n",
    "    \n",
    "    def merge_data(self):\n",
    "        \"\"\"Merge training data with features and patient records\"\"\"\n",
    "        if self.train is None or self.features is None or self.patient_notes is None:\n",
    "            print(\"Please load data first.\")\n",
    "            return None\n",
    "        \n",
    "        # Merge data\n",
    "        self.train = self.train.merge(self.features, on=['feature_num', 'case_num'], how='left')\n",
    "        self.train = self.train.merge(self.patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "        return self.train\n",
    "    \n",
    "    def check_annotation_integrity(self):\n",
    "        \"\"\"\n",
    "        Check the integrity and consistency of annotations\n",
    "        instead of manually fixing specific errors\n",
    "        \"\"\"\n",
    "        print(\"Checking annotation integrity...\")\n",
    "        \n",
    "        # Create a copy to avoid modifying original data\n",
    "        checked_train = self.train.copy()\n",
    "        \n",
    "        # Check for rows with empty annotations but location information\n",
    "        empty_annot_with_loc = checked_train[\n",
    "            (checked_train['annotation_length'] == 0) & \n",
    "            (checked_train['location'].apply(lambda x: len(x) > 0))\n",
    "        ]\n",
    "        \n",
    "        if len(empty_annot_with_loc) > 0:\n",
    "            print(f\"Warning: Found {len(empty_annot_with_loc)} rows with empty annotations but location data\")\n",
    "            \n",
    "        # Check for rows with annotations but no location information\n",
    "        annot_without_loc = checked_train[\n",
    "            (checked_train['annotation_length'] > 0) & \n",
    "            (checked_train['location'].apply(lambda x: len(x) == 0))\n",
    "        ]\n",
    "        \n",
    "        if len(annot_without_loc) > 0:\n",
    "            print(f\"Warning: Found {len(annot_without_loc)} rows with annotations but no location data\")\n",
    "        \n",
    "        # More integrity checks can be added...\n",
    "        \n",
    "        return checked_train\n",
    "    \n",
    "    def standardize_medical_text(self):\n",
    "        \"\"\"\n",
    "        Standardize medical terminology (adopted and optimized from first notebook)\n",
    "        \"\"\"\n",
    "        print(\"Standardizing medical text...\")\n",
    "        \n",
    "        # Create a working copy\n",
    "        train_standardized = self.train.copy()\n",
    "        \n",
    "        # Text standardization function\n",
    "        def standardize_text(text):\n",
    "            if pd.isna(text):\n",
    "                return text\n",
    "                \n",
    "            # Replace medical abbreviations\n",
    "            for abbr, full_form in self.medical_abbreviations.items():\n",
    "                pattern = r'\\b' + re.escape(abbr) + r'\\b'\n",
    "                text = re.sub(pattern, full_form, text, flags=re.IGNORECASE)\n",
    "                \n",
    "            return text\n",
    "        \n",
    "        # Standardize patient history text\n",
    "        train_standardized['pn_history'] = train_standardized['pn_history'].apply(standardize_text)\n",
    "        \n",
    "        # Standardize feature text\n",
    "        train_standardized['feature_text'] = train_standardized['feature_text'].apply(standardize_text)\n",
    "        \n",
    "        self.train_standardized = train_standardized\n",
    "        print(\"Medical text standardization completed\")\n",
    "        return self.train_standardized\n",
    "    \n",
    "    def correct_offsets(self):\n",
    "        \"\"\"\n",
    "        Correct annotation positions after text standardization (optimized from first notebook)\n",
    "        \"\"\"\n",
    "        print(\"Correcting annotation offsets...\")\n",
    "        \n",
    "        if not hasattr(self, 'train_standardized'):\n",
    "            print(\"Standardized data not found. Running standardize_medical_text first...\")\n",
    "            self.standardize_medical_text()\n",
    "        \n",
    "        # Create a working copy\n",
    "        train_offset_corrected = self.train_standardized.copy()\n",
    "        \n",
    "        # Since text standardization may change the text length, positions need to be updated\n",
    "        # This implementation is simplified, actual application needs more complex logic\n",
    "        \n",
    "        def adjust_location(row):\n",
    "            \"\"\"Adjust position offsets\"\"\"\n",
    "            if not row['location'] or pd.isna(row['pn_history']):\n",
    "                return row['location']\n",
    "                \n",
    "            # Get original patient history text\n",
    "            original_text = self.train.loc[row.name, 'pn_history']\n",
    "            \n",
    "            # Get standardized patient history text\n",
    "            standardized_text = row['pn_history']\n",
    "            \n",
    "            adjusted_locations = []\n",
    "            for loc_list in row['location']:\n",
    "                adjusted_loc_parts = []\n",
    "                \n",
    "                for loc in loc_list.split(';'):\n",
    "                    if ' ' in loc:\n",
    "                        start, end = map(int, loc.split())\n",
    "                        # Extract phrase from original text\n",
    "                        if start < len(original_text) and end <= len(original_text):\n",
    "                            phrase = original_text[start:end]\n",
    "                            \n",
    "                            # Find the phrase in standardized text\n",
    "                            # Note: This is a simplified method, may need more complex handling for multiple occurrences\n",
    "                            if phrase in standardized_text:\n",
    "                                new_start = standardized_text.find(phrase)\n",
    "                                new_end = new_start + len(phrase)\n",
    "                                adjusted_loc_parts.append(f\"{new_start} {new_end}\")\n",
    "                            else:\n",
    "                                # If exact match not found, use original position\n",
    "                                adjusted_loc_parts.append(loc)\n",
    "                        else:\n",
    "                            # If position out of range, use original position\n",
    "                            adjusted_loc_parts.append(loc)\n",
    "                \n",
    "                if adjusted_loc_parts:\n",
    "                    adjusted_locations.append([';'.join(adjusted_loc_parts)])\n",
    "            \n",
    "            return adjusted_locations if adjusted_locations else row['location']\n",
    "        \n",
    "        # Apply position adjustment logic\n",
    "        for i, row in train_offset_corrected.iterrows():\n",
    "            train_offset_corrected.at[i, 'location'] = adjust_location(row)\n",
    "        \n",
    "        self.train_offset_corrected = train_offset_corrected\n",
    "        print(\"Offset correction completed\")\n",
    "        return self.train_offset_corrected\n",
    "    \n",
    "    def process_spaces(self, predictions=None):\n",
    "        \"\"\"\n",
    "        Process spaces (optimized from second notebook)\n",
    "        \n",
    "        The purpose of this function is to clean up spaces in prediction labels through post-processing:\n",
    "        - Remove unnecessary leading and trailing spaces\n",
    "        - Remove middle spaces positioned before/after invalid characters (no valid characters on both sides)\n",
    "        - Preserve spaces with valid characters on both sides\n",
    "        \n",
    "        Can be used in preprocessing stage or prediction post-processing\n",
    "        \"\"\"\n",
    "        print(\"Processing spaces in text data...\")\n",
    "        \n",
    "        if not hasattr(self, 'train_offset_corrected'):\n",
    "            if hasattr(self, 'train_standardized'):\n",
    "                data = self.train_standardized\n",
    "            else:\n",
    "                data = self.train\n",
    "        else:\n",
    "            data = self.train_offset_corrected\n",
    "            \n",
    "        # Create working copy\n",
    "        processed_data = data.copy()\n",
    "        \n",
    "        def post_process_spaces(pred, text):\n",
    "            \"\"\"\n",
    "            Process prediction array to handle spaces correctly.\n",
    "            \n",
    "            Args:\n",
    "                pred: Prediction array (binary or probability values)\n",
    "                text: Corresponding text\n",
    "            \n",
    "            Returns:\n",
    "                Processed prediction array\n",
    "            \"\"\"\n",
    "            spaces = ' \\n\\r\\t'\n",
    "            \n",
    "            # Ensure lengths match\n",
    "            text = text[:len(pred)]\n",
    "            pred = pred[:len(text)]\n",
    "            \n",
    "            # Process boundary spaces\n",
    "            if text[0] in spaces:\n",
    "                pred[0] = 0\n",
    "            if text[-1] in spaces:\n",
    "                pred[-1] = 0\n",
    "\n",
    "            # Process internal spaces\n",
    "            for i in range(1, len(text) - 1):\n",
    "                if text[i] in spaces:\n",
    "                    if pred[i] and not pred[i - 1]:  # Space after invalid character\n",
    "                        pred[i] = 0\n",
    "\n",
    "                    if pred[i] and not pred[i + 1]:  # Space before invalid character\n",
    "                        pred[i] = 0\n",
    "\n",
    "                    if pred[i - 1] and pred[i + 1]:  # Space with valid characters on both sides\n",
    "                        pred[i] = 1\n",
    "            \n",
    "            return pred\n",
    "        \n",
    "        # If predictions provided, process directly\n",
    "        if predictions is not None:\n",
    "            processed_predictions = []\n",
    "            for i, pred in enumerate(predictions):\n",
    "                if i < len(processed_data):\n",
    "                    text = processed_data.iloc[i]['pn_history']\n",
    "                    processed_pred = post_process_spaces(pred, text)\n",
    "                    processed_predictions.append(processed_pred)\n",
    "                else:\n",
    "                    processed_predictions.append(pred)\n",
    "            return processed_predictions\n",
    "        \n",
    "        # Otherwise, process existing annotation locations in the data\n",
    "        # Note: This is a simplified implementation, actually needs more complex logic to handle location information\n",
    "        # Since we need to convert positions to binary array, apply space processing, then convert back to positions\n",
    "        \n",
    "        processed_locations = []\n",
    "        for i, row in processed_data.iterrows():\n",
    "            text = row['pn_history']\n",
    "            if pd.isna(text) or text == '':\n",
    "                processed_locations.append(row['location'])\n",
    "                continue\n",
    "                \n",
    "            # Create binary prediction array\n",
    "            binary_array = np.zeros(len(text))\n",
    "            \n",
    "            # Convert location information to binary array\n",
    "            for loc_list in row['location']:\n",
    "                # Fix error: handle different types of loc_list\n",
    "                if isinstance(loc_list, str):\n",
    "                    # If it's a string, split by semicolon\n",
    "                    locations = loc_list.split(';')\n",
    "                elif isinstance(loc_list, list):\n",
    "                    # If it's already a list, use directly\n",
    "                    locations = loc_list\n",
    "                else:\n",
    "                    # Skip unknown format\n",
    "                    continue\n",
    "                \n",
    "                for loc in locations:\n",
    "                    if not isinstance(loc, str):\n",
    "                        continue\n",
    "                        \n",
    "                    # Check if it contains semicolons, if so, need further splitting\n",
    "                    if ';' in loc:\n",
    "                        sub_locs = loc.split(';')\n",
    "                        for sub_loc in sub_locs:\n",
    "                            if ' ' in sub_loc:\n",
    "                                try:\n",
    "                                    start, end = map(int, sub_loc.split())\n",
    "                                    if start < len(binary_array) and end <= len(binary_array):\n",
    "                                        binary_array[start:end] = 1\n",
    "                                except ValueError:\n",
    "                                    print(f\"Warning: Cannot parse location string: {sub_loc}\")\n",
    "                    elif ' ' in loc:\n",
    "                        try:\n",
    "                            start, end = map(int, loc.split())\n",
    "                            if start < len(binary_array) and end <= len(binary_array):\n",
    "                                binary_array[start:end] = 1\n",
    "                        except ValueError:\n",
    "                            print(f\"Warning: Cannot parse location string: {loc}\")\n",
    "            \n",
    "            # Apply space processing\n",
    "            processed_binary = post_process_spaces(binary_array, text)\n",
    "            \n",
    "            # Convert processed binary array back to location information\n",
    "            new_locations = []\n",
    "            in_span = False\n",
    "            span_start = -1\n",
    "            \n",
    "            for i, val in enumerate(processed_binary):\n",
    "                if val == 1 and not in_span:\n",
    "                    # Start new span\n",
    "                    in_span = True\n",
    "                    span_start = i\n",
    "                elif val == 0 and in_span:\n",
    "                    # End current span\n",
    "                    new_locations.append(f\"{span_start} {i}\")\n",
    "                    in_span = False\n",
    "            \n",
    "            # Don't forget to handle the ending span\n",
    "            if in_span:\n",
    "                new_locations.append(f\"{span_start} {len(processed_binary)}\")\n",
    "            \n",
    "            # Update location information\n",
    "            if new_locations:\n",
    "                processed_locations.append([[';'.join(new_locations)]])\n",
    "            else:\n",
    "                processed_locations.append([])\n",
    "        \n",
    "        # Update processed data\n",
    "        for i, locs in enumerate(processed_locations):\n",
    "            if i < len(processed_data):\n",
    "                processed_data.at[i, 'location'] = locs\n",
    "        \n",
    "        # Update class attributes\n",
    "        if hasattr(self, 'train_offset_corrected'):\n",
    "            self.train_offset_corrected = processed_data\n",
    "        elif hasattr(self, 'train_standardized'):\n",
    "            self.train_standardized = processed_data\n",
    "        else:\n",
    "            self.train = processed_data\n",
    "            \n",
    "        print(\"Space processing completed\")\n",
    "        return processed_data\n",
    "    \n",
    "\n",
    "    \n",
    "    def create_folds(self, n_folds=5):\n",
    "        \"\"\"\n",
    "        Create cross-validation folds using GroupKFold (adopted from second notebook)\n",
    "        \"\"\"\n",
    "        print(f\"Creating {n_folds} folds using GroupKFold...\")\n",
    "        self.n_folds = n_folds\n",
    "        \n",
    "        if not hasattr(self, 'train_offset_corrected'):\n",
    "            if hasattr(self, 'train_standardized'):\n",
    "                data = self.train_standardized\n",
    "            else:\n",
    "                data = self.train\n",
    "        else:\n",
    "            data = self.train_offset_corrected\n",
    "        \n",
    "        # Use GroupKFold to group by pn_num\n",
    "        fold = GroupKFold(n_splits=n_folds)\n",
    "        groups = data['pn_num'].values\n",
    "        \n",
    "        for n, (train_index, val_index) in enumerate(fold.split(data, data['location'], groups)):\n",
    "            data.loc[val_index, 'fold'] = int(n)\n",
    "            \n",
    "        data['fold'] = data['fold'].astype(int)\n",
    "        \n",
    "        # Update processed data\n",
    "        if hasattr(self, 'train_offset_corrected'):\n",
    "            self.train_offset_corrected = data\n",
    "        elif hasattr(self, 'train_standardized'):\n",
    "            self.train_standardized = data\n",
    "        else:\n",
    "            self.train = data\n",
    "            \n",
    "        print(f\"Created {n_folds} folds\")\n",
    "        return data\n",
    "    \n",
    "    def create_labels_for_scoring(self, df=None):\n",
    "        \"\"\"\n",
    "        Create label format for scoring (adopted from second notebook)\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            if hasattr(self, 'train_offset_corrected'):\n",
    "                df = self.train_offset_corrected\n",
    "            elif hasattr(self, 'train_standardized'):\n",
    "                df = self.train_standardized\n",
    "            else:\n",
    "                df = self.train\n",
    "        \n",
    "        # First create standard format location lists\n",
    "        df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            lst = df.loc[i, 'location']\n",
    "            if lst:\n",
    "                # Handle different formats of location data\n",
    "                if isinstance(lst[0], list):\n",
    "                    # If it's already in list of lists format\n",
    "                    locations = []\n",
    "                    for loc_list in lst:\n",
    "                        for loc in loc_list:\n",
    "                            locations.append(loc)\n",
    "                    new_lst = ';'.join(locations)\n",
    "                elif isinstance(lst[0], str):\n",
    "                    # If it's in string list format\n",
    "                    new_lst = ';'.join(lst)\n",
    "                else:\n",
    "                    # Unknown format, skip\n",
    "                    continue\n",
    "                    \n",
    "                df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\\\"{new_lst}\\\"]]')\n",
    "        \n",
    "        # Create labels\n",
    "        truths = []\n",
    "        for location_list in df['location_for_create_labels'].values:\n",
    "            truth = []\n",
    "            if len(location_list) > 0:\n",
    "                location = location_list[0]\n",
    "                for loc in [s.split() for s in location.split(';')]:\n",
    "                    if len(loc) >= 2:  # Ensure there are start and end positions\n",
    "                        start, end = int(loc[0]), int(loc[1])\n",
    "                        truth.append([start, end])\n",
    "            truths.append(truth)\n",
    "            \n",
    "        return truths\n",
    "        \n",
    "    def pred_to_chars(self, token_type_logits, len_token, max_token, offset_mapping, text, feature_num):\n",
    "        \"\"\"\n",
    "        Convert model token-level predictions to character-level predictions\n",
    "        \n",
    "        This function handles special medical notations like \"yof\" (years old female) and \"yom\" (years old male)\n",
    "        \n",
    "        Args:\n",
    "            token_type_logits: Model's token-level predictions (logits)\n",
    "            len_token: Actual length of token sequence\n",
    "            max_token: Maximum length of token sequence\n",
    "            offset_mapping: Mapping from tokens to original text characters\n",
    "            text: Original text\n",
    "            feature_num: Current feature number being processed\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (Character-level predictions, original text)\n",
    "        \"\"\"\n",
    "        # Truncate to actual token length\n",
    "        token_type_logits = token_type_logits[:len_token]\n",
    "        offset_mapping = offset_mapping[:len_token]\n",
    "        \n",
    "        # Initialize character-level predictions\n",
    "        char_preds = np.ones(len(text)) * -1e10\n",
    "        \n",
    "        # Iterate through each token mapping\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            # Special handling for \"yof\" (age + female)\n",
    "            if text[start:end] == 'of' and start > 0 and text[start-1:end] == 'yof':\n",
    "                if feature_num in self.feature_female:\n",
    "                    # If feature is female-related, tag the last character\n",
    "                    char_preds[end-1:end] = 1\n",
    "                elif feature_num in self.feature_year:\n",
    "                    # If feature is age-related, use previous token's prediction\n",
    "                    char_preds[start:start+1] = token_type_logits[i-1]\n",
    "                else:\n",
    "                    # Otherwise, use current token's prediction\n",
    "                    char_preds[start:end] = token_type_logits[i]\n",
    "            \n",
    "            # Special handling for \"yom\" (age + male)\n",
    "            elif text[start:end] == 'om' and start > 0 and text[start-1:end] == 'yom':\n",
    "                if feature_num in self.feature_male:\n",
    "                    # If feature is male-related, tag the last character\n",
    "                    char_preds[end-1:end] = 1\n",
    "                elif feature_num in self.feature_year:\n",
    "                    # If feature is age-related, use previous token's prediction\n",
    "                    char_preds[start:start+1] = token_type_logits[i-1]\n",
    "                else:\n",
    "                    # Otherwise, use current token's prediction\n",
    "                    char_preds[start:end] = token_type_logits[i]\n",
    "            \n",
    "            # Standard handling for other tokens\n",
    "            else:\n",
    "                char_preds[start:end] = token_type_logits[i]\n",
    "                \n",
    "        return (char_preds, text)\n",
    "    \n",
    "    def create_train_test_split(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Create train/test split (if test set not provided)\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        if self.test is not None:\n",
    "            print(\"Test data already provided, skipping split.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Creating train/test split with test_size={test_size}...\")\n",
    "        \n",
    "        if hasattr(self, 'train_offset_corrected'):\n",
    "            data = self.train_offset_corrected\n",
    "        elif hasattr(self, 'train_standardized'):\n",
    "            data = self.train_standardized\n",
    "        else:\n",
    "            data = self.train\n",
    "        \n",
    "        # Use stratified sampling to maintain proportion of each pn_num\n",
    "        train_data, test_data = train_test_split(\n",
    "            data, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state,\n",
    "            stratify=data['pn_num']\n",
    "        )\n",
    "        \n",
    "        self.train_final = train_data.reset_index(drop=True)\n",
    "        self.test = test_data.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Split completed. Train: {len(self.train_final)} rows, Test: {len(self.test)} rows\")\n",
    "        return self.train_final, self.test\n",
    "    \n",
    "    def save_processed_data(self):\n",
    "        \"\"\"\n",
    "        Save processed data\n",
    "        \"\"\"\n",
    "        print(\"Saving processed data...\")\n",
    "        \n",
    "        if hasattr(self, 'train_offset_corrected'):\n",
    "            final_train = self.train_offset_corrected\n",
    "        elif hasattr(self, 'train_standardized'):\n",
    "            final_train = self.train_standardized\n",
    "        else:\n",
    "            final_train = self.train\n",
    "        \n",
    "        if not hasattr(self, 'train_final'):\n",
    "            self.train_final = final_train\n",
    "        \n",
    "        # Save training data\n",
    "        train_output_path = os.path.join(self.output_dir, \"train_processed.csv\")\n",
    "        self.train_final.to_csv(train_output_path, index=False)\n",
    "        print(f\"Saved processed train data to {train_output_path}\")\n",
    "        \n",
    "        # Save test data (if available)\n",
    "        if self.test is not None:\n",
    "            test_output_path = os.path.join(self.output_dir, \"test_processed.csv\")\n",
    "            self.test.to_csv(test_output_path, index=False)\n",
    "            print(f\"Saved processed test data to {test_output_path}\")\n",
    "        \n",
    "        # Save label information (for scoring)\n",
    "        truths = self.create_labels_for_scoring(self.train_final)\n",
    "        labels_output_path = os.path.join(self.output_dir, \"train_labels.npy\")\n",
    "        # Use dtype=object to save irregular shaped arrays\n",
    "        np.save(labels_output_path, np.array(truths, dtype=object))\n",
    "        print(f\"Saved labels to {labels_output_path}\")\n",
    "        \n",
    "        # Save fold information\n",
    "        folds_output_path = os.path.join(self.output_dir, \"folds.npy\")\n",
    "        np.save(folds_output_path, self.train_final['fold'].values)\n",
    "        print(f\"Saved fold information to {folds_output_path}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def run_full_pipeline(self):\n",
    "        \"\"\"Run the complete data processing pipeline\"\"\"\n",
    "        self.load_data()\n",
    "        self.preprocess_features()\n",
    "        self.parse_annotations()\n",
    "        self.merge_data()\n",
    "        self.check_annotation_integrity()\n",
    "        self.standardize_medical_text()\n",
    "        self.correct_offsets()\n",
    "        self.process_spaces()\n",
    "        self.create_folds(n_folds=self.n_folds)\n",
    "        if self.test is None:\n",
    "            self.create_train_test_split()\n",
    "        self.save_processed_data()\n",
    "        print(\"Full pipeline completed.\")\n",
    "        return getattr(self, 'train_final', self.train), self.test\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    processor = NBMEDataProcessor()\n",
    "    train_data, test_data = processor.run_full_pipeline()\n",
    "    print(f\"Processed train data shape: {train_data.shape}\")\n",
    "    if test_data is not None:\n",
    "        print(f\"Processed test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: List of tuples (inputs_dict, label_tensor)\n",
    "    return: (batched_inputs_dict, batched_labels_tensor)\n",
    "    \"\"\"\n",
    "    # input_dict keys should be consistent\n",
    "    inputs_keys = batch[0][0].keys()\n",
    "    batched_inputs = {\n",
    "        k: torch.stack([sample[0][k] for sample in batch], dim=0)\n",
    "        for k in inputs_keys\n",
    "    }\n",
    "    batched_labels = torch.stack([sample[1] for sample in batch], dim=0)\n",
    "    return batched_inputs, batched_labels\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Configuration\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    apex = False\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "    model = \"microsoft/deberta-v3-large\"\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0.1\n",
    "    epochs = 3\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 8\n",
    "    fc_dropout = 0.2\n",
    "    max_len = 512\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    trn_fold = [0,1]  # Can specify multiple folds for training\n",
    "    train = True\n",
    "    \n",
    "    # Data directory\n",
    "    data_dir = r\"C:\\Users\\SIMON\\Desktop\\NLP\\nbme-score-clinical-patient-notes\"\n",
    "    \n",
    "    # Adversarial training parameters\n",
    "    adv_training = True\n",
    "    adv_epsilon = 0.25\n",
    "    \n",
    "    # Focal Loss parameters\n",
    "    focal_alpha = 1\n",
    "    focal_gamma = 2\n",
    "    label_smoothing = 0.1\n",
    "    \n",
    "    # Whether to use local model\n",
    "    use_local_model = False\n",
    "    local_model_path = r\"C:\\Users\\SIMON\\Desktop\\NLP\\models\\deberta-v3-large\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./models/\"\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    CFG.trn_fold = [0]\n",
    "    \n",
    "# Ensure output directory exists\n",
    "os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "\n",
    "# —— Place at the top of the script —— \n",
    "# Automatically detect Kaggle mount path\n",
    "KAGGLE_DATA_DIR = '/kaggle/input/nbme-score-clinical-patient-notes'\n",
    "if os.path.exists(KAGGLE_DATA_DIR):\n",
    "    CFG.data_dir = KAGGLE_DATA_DIR\n",
    "    print(f\"[INFO] Using Kaggle data dir: {CFG.data_dir}\")\n",
    "\n",
    "# Optional: If you want to consolidate output to /kaggle/working\n",
    "KAGGLE_OUT_DIR = '/kaggle/working/models'\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    CFG.output_dir = KAGGLE_OUT_DIR\n",
    "    os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Using Kaggle output dir: {CFG.output_dir}\")\n",
    "\n",
    "# Optional: Local model path points to HuggingFace cache in input directory\n",
    "if CFG.use_local_model:\n",
    "    kaggle_model_dir = os.path.join('/kaggle/input', os.path.basename(CFG.local_model_path))\n",
    "    if os.path.exists(kaggle_model_dir):\n",
    "        CFG.local_model_path = kaggle_model_dir\n",
    "        print(f\"[INFO] Using Kaggle local model path: {CFG.local_model_path}\")\n",
    "\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    CFG.batch_size = 2\n",
    "    CFG.max_len     = 256\n",
    "    CFG.apex        = True\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "    print(f\"[MEMORY OPT] batch_size={CFG.batch_size}, max_len={CFG.max_len}, apex={CFG.apex}\")\n",
    "\n",
    "# ====================================================\n",
    "# Logging and Utility Functions\n",
    "# ====================================================\n",
    "def get_logger(filename=CFG.output_dir+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=CFG.seed)\n",
    "\n",
    "# ====================================================\n",
    "# Scoring Functions\n",
    "# ====================================================\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "# ====================================================\n",
    "# Helper Functions\n",
    "# ====================================================\n",
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\\\"{new_lst}\\\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                           add_special_tokens=True,\n",
    "                           return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "# ====================================================\n",
    "# Focal Loss Implementation\n",
    "# ====================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, ignore_index=-1, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: (batch*seq_len, num_classes)\n",
    "        targets: (batch*seq_len,)  where positions with pad/token_type != 0 are marked as ignore_index (e.g. -1)\n",
    "        \"\"\"\n",
    "        log_probs = F.log_softmax(inputs, dim=-1)\n",
    "\n",
    "        if self.label_smoothing > 0:\n",
    "            n_classes = inputs.size(-1)\n",
    "            # 1) To avoid errors in one_hot, clamp first\n",
    "            targets_safe = targets.clone().clamp(0, n_classes-1)\n",
    "            # 2) Construct one-hot and apply label smoothing\n",
    "            target_onehot = F.one_hot(targets_safe, n_classes).float()\n",
    "            smoothed = (1 - self.label_smoothing) * target_onehot + self.label_smoothing / n_classes\n",
    "            # 3) Calculate smoothed cross entropy\n",
    "            loss = -(smoothed * log_probs).sum(dim=-1)\n",
    "            # 4) Set ignore_index positions to 0\n",
    "            loss = loss.masked_fill(targets == self.ignore_index, 0.0)\n",
    "        else:\n",
    "            # Standard nll_loss, already supports ignore_index\n",
    "            loss = F.nll_loss(\n",
    "                log_probs, \n",
    "                targets, \n",
    "                weight=self.weight, \n",
    "                ignore_index=self.ignore_index, \n",
    "                reduction='none'\n",
    "            )\n",
    "\n",
    "        # Focal weight\n",
    "        pt = torch.exp(-loss)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * loss\n",
    "        # Average only non-ignored positions\n",
    "        valid = (targets != self.ignore_index).float()\n",
    "        return (focal * valid).sum() / valid.sum().clamp(min=1.0)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Adversarial Training Implementation (FGM)\n",
    "# ====================================================\n",
    "class FGM():\n",
    "    \"\"\"\n",
    "    Fast Gradient Method adversarial training, perturbing the model's embedding parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epsilon=0.25):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        Get adversarial samples\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = self.epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        Restore the model's original parameters\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "# ====================================================\n",
    "# Dataset and DataLoader\n",
    "# ====================================================\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = cfg.tokenizer\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self._prepare_input(\n",
    "            self.pn_historys[idx],\n",
    "            self.feature_texts[idx]\n",
    "        )\n",
    "        label = self._create_label(\n",
    "            self.pn_historys[idx],\n",
    "            self.annotation_lengths[idx],\n",
    "            self.locations[idx]\n",
    "        )\n",
    "        return inputs, label\n",
    "\n",
    "    def _prepare_input(self, text, feature_text):\n",
    "        \"\"\"\n",
    "        Encode input text, adding truncation=True to ensure maximum length\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            feature_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.cfg.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=False\n",
    "        )\n",
    "        # Convert to tensor\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "    def _create_label(self, text, annotation_length, location_list):\n",
    "        \"\"\"\n",
    "        Construct token-level labels, label length matches input length\n",
    "        \"\"\"\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.cfg.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        offset_mapping = encoded['offset_mapping']\n",
    "        # Mark non-text parts as ignore\n",
    "        ignore_idxs = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping), dtype=np.float32)\n",
    "        label[ignore_idxs] = -1\n",
    "\n",
    "        if annotation_length != 0:\n",
    "            for loc_str in location_list:\n",
    "                for loc in loc_str.split(';'):\n",
    "                    start_char, end_char = map(int, loc.split())\n",
    "                    start_idx = end_idx = None\n",
    "                    # Find corresponding token boundaries\n",
    "                    for i, (s, e) in enumerate(offset_mapping):\n",
    "                        if start_idx is None and s <= start_char < e:\n",
    "                            start_idx = i\n",
    "                        if end_idx is None and s < end_char <= e:\n",
    "                            end_idx = i + 1\n",
    "                    if start_idx is None:\n",
    "                        start_idx = end_idx\n",
    "                    if start_idx is not None and end_idx is not None:\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# ====================================================\n",
    "# Model Definition\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # —— 1. Load config with FP16 dtype —— \n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            cfg.model,\n",
    "            output_hidden_states=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # —— 2. Load pretrained model with FP16 dtype —— \n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            cfg.model,\n",
    "            config=self.config,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "        # The head layers still use FP32 (can be mixed for training)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 2)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        \n",
    "        # Use multiple dropout rates to improve stability\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 2)  # Binary classification: whether it's part of the annotation\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n",
    "\n",
    "# ====================================================\n",
    "# Training Function\n",
    "# ====================================================\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    # Create FGM adversarial training instance (if needed)\n",
    "    if CFG.adv_training:\n",
    "        fgm = FGM(model, epsilon=CFG.adv_epsilon)\n",
    "    \n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        # Normal forward pass\n",
    "        y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 2), labels.long().view(-1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1) != -1).mean()\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adversarial training\n",
    "        if CFG.adv_training:\n",
    "            fgm.attack()  # Add perturbation to embedding\n",
    "            y_preds_adv = model(inputs)\n",
    "            loss_adv = criterion(y_preds_adv.view(-1, 2), labels.long().view(-1))\n",
    "            loss_adv = torch.masked_select(loss_adv, labels.view(-1) != -1).mean()\n",
    "            if CFG.gradient_accumulation_steps > 1:\n",
    "                loss_adv = loss_adv / CFG.gradient_accumulation_steps\n",
    "            loss_adv.backward()  # Backward pass, accumulate gradient on top of normal training\n",
    "            fgm.restore()  # Restore embedding parameters\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "# ====================================================\n",
    "# Validation Function\n",
    "# ====================================================\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 2), labels.long().view(-1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        # For predictions, get the probability of the positive class\n",
    "        probs = F.softmax(y_preds, dim=2)[:, :, 1].cpu().numpy()\n",
    "        preds.append(probs)\n",
    "        \n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "# ====================================================\n",
    "# Inference Function\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        probs = F.softmax(y_preds, dim=2)[:, :, 1].cpu().numpy()\n",
    "        preds.append(probs)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "# ====================================================\n",
    "# Training Loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold, device):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Loaders\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers, \n",
    "        pin_memory=False, \n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers, \n",
    "        pin_memory=False, \n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Calculate warmup steps\n",
    "    CFG.num_warmup_steps = int(\n",
    "        CFG.num_warmup_steps * len(train_dataset) / CFG.batch_size * CFG.epochs\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Model and Optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, CFG.output_dir + 'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        return [\n",
    "            {\n",
    "                'params': [\n",
    "                    p for n, p in model.model.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                'lr': encoder_lr, \n",
    "                'weight_decay': weight_decay\n",
    "            },\n",
    "            {\n",
    "                'params': [\n",
    "                    p for n, p in model.model.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                'lr': encoder_lr, \n",
    "                'weight_decay': 0.0\n",
    "            },\n",
    "            {\n",
    "                'params': [\n",
    "                    p for n, p in model.named_parameters() \n",
    "                    if \"model\" not in n\n",
    "                ],\n",
    "                'lr': decoder_lr, \n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model,\n",
    "        encoder_lr=CFG.encoder_lr, \n",
    "        decoder_lr=CFG.decoder_lr,\n",
    "        weight_decay=CFG.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr=CFG.encoder_lr, \n",
    "        eps=CFG.eps, \n",
    "        betas=CFG.betas\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Learning Rate Scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            return get_linear_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=cfg.num_warmup_steps, \n",
    "                num_training_steps=num_train_steps\n",
    "            )\n",
    "        else:  # cosine\n",
    "            return get_cosine_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=cfg.num_warmup_steps, \n",
    "                num_training_steps=num_train_steps, \n",
    "                num_cycles=cfg.num_cycles\n",
    "            )\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    criterion = FocalLoss(\n",
    "        alpha=CFG.focal_alpha, \n",
    "        gamma=CFG.focal_gamma, \n",
    "        label_smoothing=CFG.label_smoothing\n",
    "    )\n",
    "    \n",
    "    best_score = 0.0\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        avg_loss = train_fn(\n",
    "            fold, train_loader, model, criterion, \n",
    "            optimizer, epoch, scheduler, device\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        avg_val_loss, predictions = valid_fn(\n",
    "            valid_loader, model, criterion, device\n",
    "        )\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # Scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results    = get_results(char_probs, th=0.5)\n",
    "        preds      = get_predictions(results)\n",
    "        score      = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  '\n",
    "            f'avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s'\n",
    "        )\n",
    "        LOGGER.info(f'Epoch {epoch+1} - F1 Score: {score:.4f}')\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model'\n",
    "            )\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model': model.state_dict(),\n",
    "                    'predictions': predictions\n",
    "                },\n",
    "                CFG.output_dir + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"\n",
    "            )\n",
    "\n",
    "    # —— Key modification: Explicitly disable weights_only mode when loading checkpoint —— \n",
    "    checkpoint_path = CFG.output_dir + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"\n",
    "    checkpoint = torch.load(\n",
    "        checkpoint_path,\n",
    "        map_location=torch.device('cpu'),\n",
    "        weights_only=False\n",
    "    )\n",
    "    predictions = checkpoint['predictions']\n",
    "\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n",
    "# ====================================================\n",
    "# Analysis Function for Results\n",
    "# ====================================================\n",
    "def analyze_results(oof_df, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    1) Threshold → F1 curve\n",
    "    2) F1 vs annotation length\n",
    "    3) Show 3 FPs & 3 FNs\n",
    "    \"\"\"\n",
    "    # prepare ground truth & probs\n",
    "    true_spans  = create_labels_for_scoring(oof_df)\n",
    "    preds_array = oof_df[[i for i in range(max_len)]].values\n",
    "    texts       = oof_df['pn_history'].values\n",
    "    char_probs  = get_char_probs(texts, preds_array, tokenizer)\n",
    "\n",
    "    # 1) Threshold tuning\n",
    "    ths = np.linspace(0.1, 0.9, 81)\n",
    "    f1s = []\n",
    "    for th in ths:\n",
    "        res   = get_results(char_probs, th=th)\n",
    "        predl = get_predictions(res)\n",
    "        f1s.append(get_score(true_spans, predl))\n",
    "    best_idx, best_th = int(np.argmax(f1s)), ths[np.argmax(f1s)]\n",
    "    best_f1 = f1s[best_idx]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ths, f1s)\n",
    "    plt.scatter([best_th], [best_f1])\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Threshold Tuning Curve')\n",
    "    plt.show()\n",
    "    print(f\"▶ Best threshold = {best_th:.2f}, F1 = {best_f1:.4f}\")\n",
    "\n",
    "    # 2) F1 vs. annotation length\n",
    "    best_preds = get_predictions(get_results(char_probs, th=best_th))\n",
    "    lengths    = oof_df['annotation_length'].values\n",
    "    stats = []\n",
    "    for L in sorted(set(lengths)):\n",
    "        idx = np.where(lengths == L)[0]\n",
    "        if len(idx) < 5:\n",
    "            continue\n",
    "        stats.append((L, span_micro_f1(\n",
    "            [best_preds[i] for i in idx],\n",
    "            [true_spans[i]   for i in idx]\n",
    "        )))\n",
    "    if stats:\n",
    "        xs, ys = zip(*stats)\n",
    "        plt.figure()\n",
    "        plt.plot(xs, ys)\n",
    "        plt.xlabel('Annotation Length')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 by Annotation Length')\n",
    "        plt.show()\n",
    "\n",
    "    # 3) Show error examples\n",
    "    bp = [spans_to_binary(p, len(texts[i])) for i, p in enumerate(best_preds)]\n",
    "    bt = [spans_to_binary(t, len(texts[i])) for i, t in enumerate(true_spans)]\n",
    "    fp_idx = [i for i,(b1,b2) in enumerate(zip(bp,bt)) if b1.any() and not b2.any()][:3]\n",
    "    fn_idx = [i for i,(b1,b2) in enumerate(zip(bp,bt)) if b2.any() and not b1.any()][:3]\n",
    "    error_idx = fp_idx + fn_idx\n",
    "\n",
    "    error_df = oof_df.iloc[error_idx][[\n",
    "        'pn_history','feature_text','annotation','location'\n",
    "    ]].reset_index(drop=True)\n",
    "\n",
    "    print(\"▶ Example errors (3 false positives, then 3 false negatives):\")\n",
    "    display(error_df)\n",
    "    \n",
    "# ====================================================\n",
    "# Main Function\n",
    "# ====================================================\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load data - using complete path\n",
    "    train = pd.read_csv(os.path.join(CFG.data_dir, 'train.csv'))\n",
    "    train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "    train['location'] = train['location'].apply(ast.literal_eval)\n",
    "    features = pd.read_csv(os.path.join(CFG.data_dir, 'features.csv'))\n",
    "    \n",
    "    # Preprocess features (fix any potential issues)\n",
    "    def preprocess_features(features):\n",
    "        # For example, fix specific feature text\n",
    "        features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "        return features\n",
    "    \n",
    "    features = preprocess_features(features)\n",
    "    patient_notes = pd.read_csv(os.path.join(CFG.data_dir, 'patient_notes.csv'))\n",
    "    \n",
    "    # Merge data\n",
    "    train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "    train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "    \n",
    "    # Manually correct some annotation errors\n",
    "    # ... (specific corrections can be added, such as copying from the second notebook)\n",
    "    \n",
    "    # Add annotation length field\n",
    "    train['annotation_length'] = train['annotation'].apply(len)\n",
    "    \n",
    "    # Setup Group K-Fold Cross Validation\n",
    "    Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "    groups = train['pn_num'].values\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    if CFG.use_local_model and os.path.exists(CFG.local_model_path):\n",
    "        LOGGER.info(f\"Loading tokenizer from local path: {CFG.local_model_path}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CFG.local_model_path)\n",
    "    else:\n",
    "        LOGGER.info(f\"Loading tokenizer from HuggingFace: {CFG.model}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "    CFG.tokenizer = tokenizer\n",
    "    \n",
    "    # Determine maximum length\n",
    "    # Can be adjusted based on data analysis\n",
    "    \n",
    "    # If training is needed\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                \n",
    "                # Evaluate results\n",
    "                def get_result(oof_df):\n",
    "                    labels = create_labels_for_scoring(oof_df)\n",
    "                    predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "                    char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "                    results = get_results(char_probs, th=0.49)\n",
    "                    preds = get_predictions(results)\n",
    "                    score = get_score(labels, preds)\n",
    "                    LOGGER.info(f'Score: {score:<.4f}')\n",
    "                \n",
    "                get_result(_oof_df)\n",
    "                _oof_df.to_pickle(CFG.output_dir+'oof_df_{}.pkl'.format(fold))\n",
    "        \n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        analyze_results(oof_df, CFG.tokenizer, CFG.max_len)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: List of tuples (inputs_dict, label_tensor)\n",
    "    return: (batched_inputs_dict, batched_labels_tensor)\n",
    "    \"\"\"\n",
    "    # input_dict keys should be consistent\n",
    "    inputs_keys = batch[0][0].keys()\n",
    "    batched_inputs = {\n",
    "        k: torch.stack([sample[0][k] for sample in batch], dim=0)\n",
    "        for k in inputs_keys\n",
    "    }\n",
    "    batched_labels = torch.stack([sample[1] for sample in batch], dim=0)\n",
    "    return batched_inputs, batched_labels\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Configuration\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    apex = False\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "    model = \"microsoft/deberta-v3-large\"\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0.1\n",
    "    epochs = 3\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 8\n",
    "    fc_dropout = 0.2\n",
    "    max_len = 512\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    trn_fold = [0,1]  # Can specify multiple folds for training\n",
    "    train = True\n",
    "    \n",
    "    # Data directory\n",
    "    data_dir = r\"C:\\Users\\SIMON\\Desktop\\NLP\\nbme-score-clinical-patient-notes\"\n",
    "    \n",
    "    # Adversarial training parameters\n",
    "    adv_training = True\n",
    "    adv_epsilon = 0.25\n",
    "    \n",
    "    # Focal Loss parameters\n",
    "    focal_alpha = 1\n",
    "    focal_gamma = 2\n",
    "    label_smoothing = 0.1\n",
    "    \n",
    "    # Whether to use local model\n",
    "    use_local_model = False\n",
    "    local_model_path = r\"C:\\Users\\SIMON\\Desktop\\NLP\\models\\deberta-v3-large\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = \"./models/\"\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    CFG.trn_fold = [0]\n",
    "    \n",
    "# Ensure output directory exists\n",
    "os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "\n",
    "# —— Place at the top of the script —— \n",
    "# Automatically detect Kaggle mount path\n",
    "KAGGLE_DATA_DIR = '/kaggle/input/nbme-score-clinical-patient-notes'\n",
    "if os.path.exists(KAGGLE_DATA_DIR):\n",
    "    CFG.data_dir = KAGGLE_DATA_DIR\n",
    "    print(f\"[INFO] Using Kaggle data dir: {CFG.data_dir}\")\n",
    "\n",
    "# Optional: If you want to consolidate output to /kaggle/working\n",
    "KAGGLE_OUT_DIR = '/kaggle/working/models'\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    CFG.output_dir = KAGGLE_OUT_DIR\n",
    "    os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Using Kaggle output dir: {CFG.output_dir}\")\n",
    "\n",
    "# Optional: Local model path points to HuggingFace cache in input directory\n",
    "if CFG.use_local_model:\n",
    "    kaggle_model_dir = os.path.join('/kaggle/input', os.path.basename(CFG.local_model_path))\n",
    "    if os.path.exists(kaggle_model_dir):\n",
    "        CFG.local_model_path = kaggle_model_dir\n",
    "        print(f\"[INFO] Using Kaggle local model path: {CFG.local_model_path}\")\n",
    "\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    CFG.batch_size = 2\n",
    "    CFG.max_len     = 256\n",
    "    CFG.apex        = True\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "    print(f\"[MEMORY OPT] batch_size={CFG.batch_size}, max_len={CFG.max_len}, apex={CFG.apex}\")\n",
    "\n",
    "# ====================================================\n",
    "# Logging and Utility Functions\n",
    "# ====================================================\n",
    "def get_logger(filename=CFG.output_dir+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=CFG.seed)\n",
    "\n",
    "# ====================================================\n",
    "# Scoring Functions\n",
    "# ====================================================\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "# ====================================================\n",
    "# Helper Functions\n",
    "# ====================================================\n",
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\\\"{new_lst}\\\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                           add_special_tokens=True,\n",
    "                           return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "# ====================================================\n",
    "# Focal Loss Implementation\n",
    "# ====================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, ignore_index=-1, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: (batch*seq_len, num_classes)\n",
    "        targets: (batch*seq_len,)  where positions with pad/token_type != 0 are marked as ignore_index (e.g. -1)\n",
    "        \"\"\"\n",
    "        log_probs = F.log_softmax(inputs, dim=-1)\n",
    "\n",
    "        if self.label_smoothing > 0:\n",
    "            n_classes = inputs.size(-1)\n",
    "            # 1) To avoid errors in one_hot, clamp first\n",
    "            targets_safe = targets.clone().clamp(0, n_classes-1)\n",
    "            # 2) Construct one-hot and apply label smoothing\n",
    "            target_onehot = F.one_hot(targets_safe, n_classes).float()\n",
    "            smoothed = (1 - self.label_smoothing) * target_onehot + self.label_smoothing / n_classes\n",
    "            # 3) Calculate smoothed cross entropy\n",
    "            loss = -(smoothed * log_probs).sum(dim=-1)\n",
    "            # 4) Set ignore_index positions to 0\n",
    "            loss = loss.masked_fill(targets == self.ignore_index, 0.0)\n",
    "        else:\n",
    "            # Standard nll_loss, already supports ignore_index\n",
    "            loss = F.nll_loss(\n",
    "                log_probs, \n",
    "                targets, \n",
    "                weight=self.weight, \n",
    "                ignore_index=self.ignore_index, \n",
    "                reduction='none'\n",
    "            )\n",
    "\n",
    "        # Focal weight\n",
    "        pt = torch.exp(-loss)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * loss\n",
    "        # Average only non-ignored positions\n",
    "        valid = (targets != self.ignore_index).float()\n",
    "        return (focal * valid).sum() / valid.sum().clamp(min=1.0)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Adversarial Training Implementation (FGM)\n",
    "# ====================================================\n",
    "class FGM():\n",
    "    \"\"\"\n",
    "    Fast Gradient Method adversarial training, perturbing the model's embedding parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epsilon=0.25):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        Get adversarial samples\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = self.epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        Restore the model's original parameters\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "# ====================================================\n",
    "# Dataset and DataLoader\n",
    "# ====================================================\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = cfg.tokenizer\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self._prepare_input(\n",
    "            self.pn_historys[idx],\n",
    "            self.feature_texts[idx]\n",
    "        )\n",
    "        label = self._create_label(\n",
    "            self.pn_historys[idx],\n",
    "            self.annotation_lengths[idx],\n",
    "            self.locations[idx]\n",
    "        )\n",
    "        return inputs, label\n",
    "\n",
    "    def _prepare_input(self, text, feature_text):\n",
    "        \"\"\"\n",
    "        Encode input text, adding truncation=True to ensure maximum length\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            feature_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.cfg.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=False\n",
    "        )\n",
    "        # Convert to tensor\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "    def _create_label(self, text, annotation_length, location_list):\n",
    "        \"\"\"\n",
    "        Construct token-level labels, label length matches input length\n",
    "        \"\"\"\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.cfg.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        offset_mapping = encoded['offset_mapping']\n",
    "        # Mark non-text parts as ignore\n",
    "        ignore_idxs = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping), dtype=np.float32)\n",
    "        label[ignore_idxs] = -1\n",
    "\n",
    "        if annotation_length != 0:\n",
    "            for loc_str in location_list:\n",
    "                for loc in loc_str.split(';'):\n",
    "                    start_char, end_char = map(int, loc.split())\n",
    "                    start_idx = end_idx = None\n",
    "                    # Find corresponding token boundaries\n",
    "                    for i, (s, e) in enumerate(offset_mapping):\n",
    "                        if start_idx is None and s <= start_char < e:\n",
    "                            start_idx = i\n",
    "                        if end_idx is None and s < end_char <= e:\n",
    "                            end_idx = i + 1\n",
    "                    if start_idx is None:\n",
    "                        start_idx = end_idx\n",
    "                    if start_idx is not None and end_idx is not None:\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# ====================================================\n",
    "# Model Definition\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # —— 1. Load config with FP16 dtype —— \n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            cfg.model,\n",
    "            output_hidden_states=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # —— 2. Load pretrained model with FP16 dtype —— \n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            cfg.model,\n",
    "            config=self.config,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "        # The head layers still use FP32 (can be mixed for training)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 2)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        \n",
    "        # Use multiple dropout rates to improve stability\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 2)  # Binary classification: whether it's part of the annotation\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n",
    "\n",
    "# ====================================================\n",
    "# Training Function\n",
    "# ====================================================\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    # Create FGM adversarial training instance (if needed)\n",
    "    if CFG.adv_training:\n",
    "        fgm = FGM(model, epsilon=CFG.adv_epsilon)\n",
    "    \n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        # Normal forward pass\n",
    "        y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 2), labels.long().view(-1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1) != -1).mean()\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adversarial training\n",
    "        if CFG.adv_training:\n",
    "            fgm.attack()  # Add perturbation to embedding\n",
    "            y_preds_adv = model(inputs)\n",
    "            loss_adv = criterion(y_preds_adv.view(-1, 2), labels.long().view(-1))\n",
    "            loss_adv = torch.masked_select(loss_adv, labels.view(-1) != -1).mean()\n",
    "            if CFG.gradient_accumulation_steps > 1:\n",
    "                loss_adv = loss_adv / CFG.gradient_accumulation_steps\n",
    "            loss_adv.backward()  # Backward pass, accumulate gradient on top of normal training\n",
    "            fgm.restore()  # Restore embedding parameters\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "# ====================================================\n",
    "# Validation Function\n",
    "# ====================================================\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 2), labels.long().view(-1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        # For predictions, get the probability of the positive class\n",
    "        probs = F.softmax(y_preds, dim=2)[:, :, 1].cpu().numpy()\n",
    "        preds.append(probs)\n",
    "        \n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "# ====================================================\n",
    "# Inference Function\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        probs = F.softmax(y_preds, dim=2)[:, :, 1].cpu().numpy()\n",
    "        preds.append(probs)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "# ====================================================\n",
    "# Training Loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold, device):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Loaders\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers, \n",
    "        pin_memory=False, \n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers, \n",
    "        pin_memory=False, \n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Calculate warmup steps\n",
    "    CFG.num_warmup_steps = int(\n",
    "        CFG.num_warmup_steps * len(train_dataset) / CFG.batch_size * CFG.epochs\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Model and Optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, CFG.output_dir + 'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        return [\n",
    "            {\n",
    "                'params': [\n",
    "                    p for n, p in model.model.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                'lr': encoder_lr, \n",
    "                'weight_decay': weight_decay\n",
    "            },\n",
    "            {\n",
    "                'params': [\n",
    "                    p for n, p in model.model.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                'lr': encoder_lr, \n",
    "                'weight_decay': 0.0\n",
    "            },\n",
    "            {\n",
    "                'params': [\n",
    "                    p for n, p in model.named_parameters() \n",
    "                    if \"model\" not in n\n",
    "                ],\n",
    "                'lr': decoder_lr, \n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model,\n",
    "        encoder_lr=CFG.encoder_lr, \n",
    "        decoder_lr=CFG.decoder_lr,\n",
    "        weight_decay=CFG.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr=CFG.encoder_lr, \n",
    "        eps=CFG.eps, \n",
    "        betas=CFG.betas\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Learning Rate Scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            return get_linear_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=cfg.num_warmup_steps, \n",
    "                num_training_steps=num_train_steps\n",
    "            )\n",
    "        else:  # cosine\n",
    "            return get_cosine_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=cfg.num_warmup_steps, \n",
    "                num_training_steps=num_train_steps, \n",
    "                num_cycles=cfg.num_cycles\n",
    "            )\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    criterion = FocalLoss(\n",
    "        alpha=CFG.focal_alpha, \n",
    "        gamma=CFG.focal_gamma, \n",
    "        label_smoothing=CFG.label_smoothing\n",
    "    )\n",
    "    \n",
    "    best_score = 0.0\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        avg_loss = train_fn(\n",
    "            fold, train_loader, model, criterion, \n",
    "            optimizer, epoch, scheduler, device\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        avg_val_loss, predictions = valid_fn(\n",
    "            valid_loader, model, criterion, device\n",
    "        )\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # Scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results    = get_results(char_probs, th=0.5)\n",
    "        preds      = get_predictions(results)\n",
    "        score      = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  '\n",
    "            f'avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s'\n",
    "        )\n",
    "        LOGGER.info(f'Epoch {epoch+1} - F1 Score: {score:.4f}')\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model'\n",
    "            )\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model': model.state_dict(),\n",
    "                    'predictions': predictions\n",
    "                },\n",
    "                CFG.output_dir + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"\n",
    "            )\n",
    "\n",
    "    # —— Key modification: Explicitly disable weights_only mode when loading checkpoint —— \n",
    "    checkpoint_path = CFG.output_dir + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"\n",
    "    checkpoint = torch.load(\n",
    "        checkpoint_path,\n",
    "        map_location=torch.device('cpu'),\n",
    "        weights_only=False\n",
    "    )\n",
    "    predictions = checkpoint['predictions']\n",
    "\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n",
    "# ====================================================\n",
    "# Analysis Function for Results\n",
    "# ====================================================\n",
    "def analyze_results(oof_df, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    1) Threshold → F1 curve\n",
    "    2) F1 vs annotation length\n",
    "    3) Show 3 FPs & 3 FNs\n",
    "    \"\"\"\n",
    "    # prepare ground truth & probs\n",
    "    true_spans  = create_labels_for_scoring(oof_df)\n",
    "    preds_array = oof_df[[i for i in range(max_len)]].values\n",
    "    texts       = oof_df['pn_history'].values\n",
    "    char_probs  = get_char_probs(texts, preds_array, tokenizer)\n",
    "\n",
    "    # 1) Threshold tuning\n",
    "    ths = np.linspace(0.1, 0.9, 81)\n",
    "    f1s = []\n",
    "    for th in ths:\n",
    "        res   = get_results(char_probs, th=th)\n",
    "        predl = get_predictions(res)\n",
    "        f1s.append(get_score(true_spans, predl))\n",
    "    best_idx, best_th = int(np.argmax(f1s)), ths[np.argmax(f1s)]\n",
    "    best_f1 = f1s[best_idx]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ths, f1s)\n",
    "    plt.scatter([best_th], [best_f1])\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Threshold Tuning Curve')\n",
    "    plt.show()\n",
    "    print(f\"▶ Best threshold = {best_th:.2f}, F1 = {best_f1:.4f}\")\n",
    "\n",
    "    # 2) F1 vs. annotation length\n",
    "    best_preds = get_predictions(get_results(char_probs, th=best_th))\n",
    "    lengths    = oof_df['annotation_length'].values\n",
    "    stats = []\n",
    "    for L in sorted(set(lengths)):\n",
    "        idx = np.where(lengths == L)[0]\n",
    "        if len(idx) < 5:\n",
    "            continue\n",
    "        stats.append((L, span_micro_f1(\n",
    "            [best_preds[i] for i in idx],\n",
    "            [true_spans[i]   for i in idx]\n",
    "        )))\n",
    "    if stats:\n",
    "        xs, ys = zip(*stats)\n",
    "        plt.figure()\n",
    "        plt.plot(xs, ys)\n",
    "        plt.xlabel('Annotation Length')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 by Annotation Length')\n",
    "        plt.show()\n",
    "\n",
    "    # 3) Show error examples\n",
    "    bp = [spans_to_binary(p, len(texts[i])) for i, p in enumerate(best_preds)]\n",
    "    bt = [spans_to_binary(t, len(texts[i])) for i, t in enumerate(true_spans)]\n",
    "    fp_idx = [i for i,(b1,b2) in enumerate(zip(bp,bt)) if b1.any() and not b2.any()][:3]\n",
    "    fn_idx = [i for i,(b1,b2) in enumerate(zip(bp,bt)) if b2.any() and not b1.any()][:3]\n",
    "    error_idx = fp_idx + fn_idx\n",
    "\n",
    "    error_df = oof_df.iloc[error_idx][[\n",
    "        'pn_history','feature_text','annotation','location'\n",
    "    ]].reset_index(drop=True)\n",
    "\n",
    "    print(\"▶ Example errors (3 false positives, then 3 false negatives):\")\n",
    "    display(error_df)\n",
    "    \n",
    "# ====================================================\n",
    "# Main Function\n",
    "# ====================================================\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load data - using complete path\n",
    "    train = pd.read_csv(os.path.join(CFG.data_dir, 'train.csv'))\n",
    "    train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "    train['location'] = train['location'].apply(ast.literal_eval)\n",
    "    features = pd.read_csv(os.path.join(CFG.data_dir, 'features.csv'))\n",
    "    \n",
    "    # Preprocess features (fix any potential issues)\n",
    "    def preprocess_features(features):\n",
    "        # For example, fix specific feature text\n",
    "        features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "        return features\n",
    "    \n",
    "    features = preprocess_features(features)\n",
    "    patient_notes = pd.read_csv(os.path.join(CFG.data_dir, 'patient_notes.csv'))\n",
    "    \n",
    "    # Merge data\n",
    "    train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "    train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "    \n",
    "    # Manually correct some annotation errors\n",
    "    # ... (specific corrections can be added, such as copying from the second notebook)\n",
    "    \n",
    "    # Add annotation length field\n",
    "    train['annotation_length'] = train['annotation'].apply(len)\n",
    "    \n",
    "    # Setup Group K-Fold Cross Validation\n",
    "    Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "    groups = train['pn_num'].values\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    if CFG.use_local_model and os.path.exists(CFG.local_model_path):\n",
    "        LOGGER.info(f\"Loading tokenizer from local path: {CFG.local_model_path}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CFG.local_model_path)\n",
    "    else:\n",
    "        LOGGER.info(f\"Loading tokenizer from HuggingFace: {CFG.model}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "    CFG.tokenizer = tokenizer\n",
    "    \n",
    "    # Determine maximum length\n",
    "    # Can be adjusted based on data analysis\n",
    "    \n",
    "    # If training is needed\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold, device)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                \n",
    "                # Evaluate results\n",
    "                def get_result(oof_df):\n",
    "                    labels = create_labels_for_scoring(oof_df)\n",
    "                    predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "                    char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "                    results = get_results(char_probs, th=0.49)\n",
    "                    preds = get_predictions(results)\n",
    "                    score = get_score(labels, preds)\n",
    "                    LOGGER.info(f'Score: {score:<.4f}')\n",
    "                \n",
    "                get_result(_oof_df)\n",
    "                _oof_df.to_pickle(CFG.output_dir+'oof_df_{}.pkl'.format(fold))\n",
    "        \n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        analyze_results(oof_df, CFG.tokenizer, CFG.max_len)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
